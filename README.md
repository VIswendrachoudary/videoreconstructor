#ğŸ¥ Jumbled Video Reconstruction Challenge

This project restores the original order of a **10-second, 1080p (30 FPS)** video whose frames have been randomly shuffled.  
It leverages **deep visual similarity (ResNet-18)** and **feature coherence** to rebuild the temporal sequence accurately.

---

## ğŸ§© Objective

Given a shuffled video (`data/jumbled_video.mp4`), the system:

1. Extracts individual frames  
2. Generates visual embeddings using a pretrained **ResNet-18** model  
3. Infers the correct sequential frame order  
4. Rebuilds the video in its original motion flow  

---

## âš™ï¸ Requirements

**Python 3.8 +**

Install dependencies before running any scripts:

```bash
pip install opencv-python tqdm numpy torch torchvision pillow
ğŸ“ Project Structure
kotlin
Copy code
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ jumbled_video.mp4
â”‚   â”œâ”€â”€ frames_jumbled/
â”‚   â”œâ”€â”€ frame_features.npy
â”‚   â””â”€â”€ frame_order_final.npy
â”‚
â”œâ”€â”€ output/
â”‚   â””â”€â”€ reconstructed_video_final_smooth.mp4
â”‚
â”œâ”€â”€ extract_frames.py
â”œâ”€â”€ extract_features.py
â”œâ”€â”€ reconstruct_sequence.py
â”œâ”€â”€ rebuild_video.py
â””â”€â”€ README.md
ğŸ’¡ Tip:
If data/ or output/ folders donâ€™t exist, create them manually before running the scripts.

ğŸš€ Usage & Execution Order
1ï¸âƒ£ Extract Frames
Extract all frames from the input video.

bash
Copy code
python extract_frames.py
Input: data/jumbled_video.mp4
Output: data/frames_jumbled/frame_0000.jpg â€¦

2ï¸âƒ£ Extract Frame Features
Generate and save 512-D feature vectors using ResNet-18.

bash
Copy code
python extract_features.py
Input: data/frames_jumbled/
Output: data/frame_features.npy

3ï¸âƒ£ Generate Frame Order
Create the correct order file (frame_order_final.npy).
This step may use custom logic or a separate similarity model.

bash
Copy code
python reconstruct_sequence.py
Input: data/frames_jumbled/
Output: data/frame_order_final.npy

4ï¸âƒ£ Rebuild the Video
Reconstruct the final smooth video in the correct order.

bash
Copy code
python rebuild_video.py
Inputs:

data/frames_jumbled/

data/frame_order_final.npy

Output:
output/reconstructed_video_final_smooth.mp4

Execution time is logged automatically in execution_time.txt.

ğŸ§  Key Design Choices
Aspect	Decision	Purpose
Feature Embedding	ResNet-18 (ImageNet pretrained)	Captures spatial-semantic frame similarity
Matching Logic	Pairwise feature similarity	Infers temporal coherence
Optimization	GPU acceleration, batch processing	Speed & scalability
Modularity	Independent stages	Easy debugging & upgrades

ğŸ§® Complexity (Approx.)
Stage	Time Complexity
Feature extraction	O(N)
Similarity computation	O(NÂ²)
Sorting / reconstruction	O(N log N)

For 300 frames, runtime typically stays within a few seconds on a GPU-enabled system.

âš¡ Optimization & Parallelism
CUDA auto-use when available

Batch inference for efficient tensor ops

Vectorized NumPy for similarity computation

Ready for multiprocessing extensions

ğŸ§© Limitations & Future Work
Nearly identical frames can confuse ordering

Optical flow or temporal CNNs could enhance stability

Graph-based order inference may yield global consistency

ğŸ¯ Output
âœ… Reconstructed video: output/reconstructed_video_final_smooth.mp4
ğŸ•’ Execution log: execution_time.txt
ğŸ“Š Metrics: Frame continuity, average similarity (%), runtime efficiency

ğŸ Result
The system reconstructs the shuffled video with near-original motion, preserving smooth transitions and temporal integrity.

ğŸ“˜ Algorithm Explanation
1ï¸âƒ£ Overview
Reconstruct a shuffled 300-frame video (30 FPS) by inferring temporal relations purely from frame content.

2ï¸âƒ£ Approach
Step 1 â€“ Frame Extraction
Using OpenCV, every frame is stored sequentially for processing.

Step 2 â€“ Feature Extraction
Each frame passes through ResNet-18 (pretrained on ImageNet).
The classifier head is removed to obtain a 512-D visual embedding per frame.
All embeddings are saved in frame_features.npy.

Step 3 â€“ Sequence Reconstruction
A predicted order (frame_order_final.npy) is generated based on pairwise feature similarity â€”
frames that look alike and follow motion cues are placed adjacently.
Local smoothing fixes sharp jumps; reversal detection ensures forward playback.

Step 4 â€“ Video Rebuilding
Frames are reassembled using OpenCVâ€™s VideoWriter.
Frame continuity and order statistics are printed for quick verification.

3ï¸âƒ£ Optimization & Parallelism
GPU Acceleration: CUDA used automatically when available

Batch Processing: Reduces read/write overhead

Vectorized Math: NumPy replaces explicit Python loops

Parallel Extensions: Ready for multiprocessing on larger datasets

4ï¸âƒ£ Design Considerations
Aspect	Decision	Reason
Accuracy	Deep visual embeddings	Capture semantics & color
Speed	ResNet-18 backbone	Balance detail vs. runtime
Robustness	Order smoothing & reversal check	Avoid local misalignments
Scalability	Modular design	Easy extension & parallelization

5ï¸âƒ£ Time Complexity (Approx.)
Stage	Complexity
Feature extraction	O(N)
Pairwise similarity	O(NÂ²)
Sorting / reconstruction	O(N log N)

For 300 frames, runtime stays within a few seconds on the benchmark system.

6ï¸âƒ£ Limitations & Future Work
Nearly identical frames can confuse ordering

Could be enhanced with optical flow or temporal CNNs

Future versions may use graph traversal for global order inference

7ï¸âƒ£ Result
The reconstructed video closely matches the original temporal flow, maintaining natural motion and minimizing frame discontinuities.

End of Document âœ…












C
